report {
    overwrite = true
}
timeline {
    overwrite = true
}
trace {
    overwrite = true
}

params {
    input                 = "${projectDir}/example/genotype/example.vcf/chr2[0-2].vcf"
    covarfile             = "${projectDir}/example/covariates.tsv"
    phenofile             = "${projectDir}/example/phenotype.cs.tsv"

    //Variables names
    pheno_name            = 'y'
    covariates            = 'SEX age_at_baseline'
    study_col             = 'study_arm'
    time_col              = 'study_days'

    // Model variables
    longitudinal_flag     = false
    survival_flag         = false
    linear_flag           = true
    chunk_flag            = true
    chunk_size            = 30000

    //# Parameters for genetic QC
    r2thres               = -9
    minor_allele_freq     = '0.05'
    minor_allele_ct       = '20'
    kinship               = '0.177'
    ancestry              = 'EUR'
    assembly              = 'hg19'
    
    //#Identifier for the input genotype files - useful to cache results
    dataset               = 'TEST'

    //# Generate manhattan with result files
    mh_plot               = true

    // Storage root - can be local path or GCS bucket
    // For local: defaults to projectDir (where main.nf is located)
    // For cloud: export STORE_ROOT=gs://your-bucket
    STORE_ROOT = System.getenv('STORE_ROOT') ?: projectDir
    PROJECT_NAME  = System.getenv('PROJECT_NAME') ?: 'unnamed_project'
    
    // Reference genomes directory (mounted from host, in pipeline root directory)
    reference_dir = System.getenv('REFERENCE_DIR') ?: "${projectDir}/References"
    
    // Derived paths
    project_dir = "${STORE_ROOT}/${PROJECT_NAME}"
}

// Common environment variables used across all profiles
def commonEnv = [
    RESOURCE_DIR: "/workspace/References",
    OUTPUT_DIR: "${params.project_dir}/results",
    STORE_DIR: "${params.project_dir}/cache"
]

// Create profiles
profiles {
  standard {
    env = commonEnv
    workDir = "${params.project_dir}/work"
    process {
      container = 'hirotakai/longgwas:v2.0.1'
      containerOptions = "-v ${params.reference_dir}:/workspace/References"
      cpus = 2
      withLabel: small {
        cpus = 2
        memory = '6 GB'
      }
      withLabel: medium {
        cpus = 4
        memory = '12 GB'
      }
      withLabel: large_mem {
        cpus = 4
        memory = '12 GB'
      }
    }
    docker {
      enabled = true
      temp = 'auto'
    }
  }

  // Local test profile using newly built Docker image
  // Note: bin/ scripts are auto-mounted by Nextflow, so you can modify them without rebuilding Docker
  // Reference files are mounted from host (downloaded on first run)
  localtest {
    env = commonEnv
    workDir = "${params.project_dir}/work"
    process {
      container = 'longgwas:slim'
      containerOptions = "-v ${params.reference_dir}:/workspace/References"
      cpus = 2
      withLabel: small {
        cpus = 2
        memory = '6 GB'
      }
      withLabel: medium {
        cpus = 4
        memory = '12 GB'
      }
      withLabel: large_mem {
        cpus = 4
        memory = '12 GB'
      }
    }
    docker {
      enabled = true
      temp = 'auto'
    }
  }

  biowulf {
    env = commonEnv
    workDir = "${params.project_dir}/work"
    process {
      executor = 'slurm'
      queue = 'norm'
      container = "${params.STORE_ROOT}/Docker/gwas-pipeline_survival.sif"
      cpus = 2
      clusterOptions = '--gres=lscratch:10'

      withLabel: small {
        cpus = 2
        memory = '5 GB'
        time = '2h'
      }
      withLabel: medium {
        cpus = 2
        memory = '15 GB'
        time = '4h'
      }
      withLabel: large_mem {
        cpus = 10
        memory = '115 GB'
        time = '8h'
      }
    }
    executor {
      name = 'slurm'
      pollInterval = '2 min'
      queueSize = 200
      queueStatInterval = '5 min'
      submitRateLimit = '6/1min'
    }
    singularity {
      enabled = true
      autoMounts = true
      runOptions = "--bind ${projectDir} --bind ${params.reference_dir}:/workspace/References --env APPEND_PATH=${projectDir}/bin"
    }
  }

  // For running on already allocated resources (sinteractive/sbatch)
  biowulflocal {
    env = commonEnv
    workDir = "${params.project_dir}/work"
    process {
      executor = 'local'
      container = "${params.STORE_ROOT}/Docker/gwas-pipeline_survival.sif"
      maxForks = 2  // Limit parallel tasks to match your allocation

      withLabel: small {
        cpus = 2
        memory = '5 GB'
      }
      withLabel: medium {
        cpus = 2
        memory = '15 GB'
      }
      withLabel: large_mem {
        cpus = 2  // Limited to prevent overload
        memory = '50 GB'
      }
    }
    singularity {
      enabled = true
      autoMounts = true
      runOptions = "--bind ${projectDir} --bind ${params.reference_dir}:/workspace/References --env APPEND_PATH=${projectDir}/bin"
    }
  }

  gcb {
    env = commonEnv

    workDir = "${params.project_dir}/work"
    
    // Enable detailed error reporting
    cleanup = false  // Keep work files even on success for debugging
    
    process {
      executor = 'google-batch'
      container = 'hirotakai/longgwas:v2.0.1'
      containerOptions = "-v ${params.reference_dir}:/workspace/References"
      errorStrategy = { task.exitStatus==14 ? 'retry' : 'terminate' }
      maxRetries = 5
      cpus = 2
      memory = '8 GB'
      
      // Capture stdout/stderr to work directory for debugging
      beforeScript = '''
        echo "=== Task Environment ===" >&2
        echo "Task started: $(date)" >&2
        echo "Hostname: $(hostname)" >&2
        echo "CPUs allocated: $(nproc)" >&2
        echo "Memory total: $(free -h | awk '/^Mem:/ {print $2}')" >&2
        echo "Memory available: $(free -h | awk '/^Mem:/ {print $7}')" >&2
        echo "Disk space: $(df -h . | tail -1 | awk '{print $4 " available of " $2}')" >&2
        echo "======================" >&2
      '''.stripIndent()
      afterScript = '''
        EXIT_CODE=$?
        echo "=== Task Completion ===" >&2
        echo "Task finished: $(date)" >&2
        echo "Exit code: $EXIT_CODE" >&2
        echo "Memory used: $(free -h | awk '/^Mem:/ {print $3}')" >&2
        echo "Disk used: $(df -h . | tail -1 | awk '{print $3 " used, " $4 " available"}')" >&2
        echo "======================" >&2
        exit $EXIT_CODE
      '''.stripIndent()
      
      withLabel: small {
        cpus = 2
        memory = '8 GB'
      }
      withLabel: medium {
        cpus = 4
        memory = '16 GB'
      }
      withLabel: large_mem {
        cpus = 4
        memory = '32 GB'
      }
    }

    google.region  = 'europe-west4'
    google.project = "${GOOGLE_CLOUD_PROJECT}"

    google.batch.serviceAccountEmail = "${GOOGLE_SERVICE_ACCOUNT_EMAIL}"
    google.batch.usePrivateAddress = true
    google.batch.network = 'global/networks/network'
    google.batch.subnetwork = "regions/europe-west4/subnetworks/subnetwork"
    google.batch.bootDiskSize = '50 GB'
    
    // Allow Google Batch to execute scripts properly
    google.batch.allowedLocations = ['zones/europe-west4-a', 'zones/europe-west4-b']
  }

  adwb {
    env = commonEnv
    workDir = "${params.project_dir}/work"
    process {
      container = 'gwas-pipeline'
      containerOptions = "-v ${params.reference_dir}:/workspace/References"
      cpus = 2

      withLabel: small {
        cpus = 2
        memory = '5 GB'
      }
      withLabel: medium {
        cpus = 2
        memory = '15 GB'
      }
      withLabel: large_mem {
        cpus = 10
        memory = '70 GB'
      }
    }
    executor {
      cpus = 20
      name = 'local'
      memory = '75 GB'
    }
    docker {
      enabled = true
      temp = 'auto'
    }
  }
}
